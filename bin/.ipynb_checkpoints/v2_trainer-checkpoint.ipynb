{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dbe125c-7cf1-4af7-a9b9-15170b271db7",
   "metadata": {},
   "source": [
    "# Trainer v2\n",
    "- training from notebook  \n",
    "- txt logger\n",
    "- json logger\n",
    "- modules\n",
    "    - lib_util\n",
    "    - lib_logger\n",
    "    - lib_keras\n",
    "    - lig_dataset\n",
    "    - lib_plot\n",
    "- todo: continue training by loading weights\n",
    "- todo: training from python file (trainer.py --source anyjson.json)\n",
    "- todo: sending stats to mqtt broker\n",
    "- todo: calling trainer.py as service request\n",
    "- todo: creating web UI for send training request and receiving live (partial) resutls\n",
    "- todo: creating docker base service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04897f9e-0dfb-47a3-b5b6-29a33340eeb3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661e7371-4255-4a8f-a377-2ec87c35d6f9",
   "metadata": {},
   "source": [
    "# Trainer v2\n",
    "- training from notebook  \n",
    "- txt logger\n",
    "- json logger\n",
    "- modules\n",
    "    - lib_util\n",
    "    - lib_logger\n",
    "    - lib_keras\n",
    "    - lig_dataset\n",
    "    - lib_plot\n",
    "- todo: continue training by loading weights\n",
    "- todo: training from python file (trainer.py --source anyjson.json)\n",
    "- todo: sending stats to mqtt broker\n",
    "- todo: calling trainer.py as service request\n",
    "- todo: creating web UI for send training request and receiving live (partial) resutls\n",
    "- todo: creating docker base service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3fec06e-ba8d-4527-80a0-88075c400594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\" # or any {'0', '1', '2','3'}\n",
    "\n",
    "from lib_utils import utilities\n",
    "from lib_logger import logger\n",
    "from lib_keras import keras_applications\n",
    "from lib_dataset import Dataset\n",
    "from lib_plot import show_training_performance,show_dataset_chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9200413-87be-4ac0-b66b-82d71d1a7295",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Definitions \n",
    "- Read json file: fetch all user paramters into USER_PARA 1. Read json file: fetch all user paramters into USER_PARA \n",
    "\n",
    "**Action**: User interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a80cb9-0b3d-4ab4-9c42-d13c786b5645",
   "metadata": {},
   "outputs": [],
   "source": [
    "util=utilities()\n",
    "#USER_PARA=util.load_JSON_file('conf-montgomeryset.json')\n",
    "USER_PARA=util.load_JSON_file('exp03_covid19_efficientnet.json')\n",
    "prefix=USER_PARA['prefix']\n",
    "\n",
    "color_mode=USER_PARA[\"colorMode\"]\n",
    "imgChannel= 1 if color_mode=='grayscale' else 3\n",
    "image_size=(USER_PARA['imgHeight'],USER_PARA['imgWidth'])\n",
    "\n",
    "image_shape=(USER_PARA['imgHeight'],USER_PARA['imgWidth'],imgChannel)\n",
    "\n",
    "logFilenameJSON=os.path.join(prefix,\"log.json\")\n",
    "logFilenameTXT=os.path.join(prefix,\"log.txt\")\n",
    "\n",
    "base_architecture=USER_PARA['architecture']\n",
    "\n",
    "root_dataset=USER_PARA[\"root_dataset\"]\n",
    "\n",
    "batch_size=USER_PARA[\"batch_size\"]\n",
    "dataset_tag=USER_PARA['dataset_tag']\n",
    "epochs=USER_PARA['epochs']\n",
    "\n",
    "callbacks_enabled={\"CSVLogger\":True,\n",
    "                     \"TensorBoard\":True,\n",
    "                     \"ModelCheckpoint\":True,\n",
    "                     \"EarlyStopping\":True,\n",
    "                     \"TerminateOnNaN\":True,\n",
    "                     \"ReduceLROnPlateau\":True,\n",
    "                     \"LearningRateScheduler\":False,\n",
    "                     \"RemoteMonitor\":False}\n",
    "_exception={}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced65b35-390b-4775-b29f-4b60d1db9036",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0239ec0d-54dd-45b5-b65f-44c25641e5db",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Parameters\n",
    "- Create root directory on PARA.prefix.  \n",
    "- Start logging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2aef1b1-3dab-409f-8d40-aabeec012720",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(prefix):\n",
    "    os.mkdir(prefix)\n",
    "\n",
    "path_to_image=os.path.join(prefix,'image')\n",
    "if not os.path.isdir(path_to_image):\n",
    "    os.mkdir(path_to_image)\n",
    "\n",
    "\n",
    "log_json=logger(logFilenameJSON)\n",
    "log_txt=logger(logFilenameTXT)\n",
    "\n",
    "log_txt.print_log(\"Getting Starting...\",overwrite=True)\n",
    "log_txt.print_log('\\tprefix:'+prefix)\n",
    "\n",
    "log_json.print_jsonlog({'prefix':prefix},overwrite=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d3fe98-fb3c-4a53-96c1-a0fe4f90778d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dataset  \n",
    "- load the data from directory\n",
    "- Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791d74b3-f9dd-4aff-be7c-0f73879ea459",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5100bf-f0ec-407d-858e-95223056288c",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_txt.print_log('Loading Dataset...')\n",
    "datasetOBJ=Dataset(dataset_tag=dataset_tag,\n",
    "             path_to_dataset=root_dataset,\n",
    "             image_size=image_size, \n",
    "             color_mode=color_mode, \n",
    "             batch_size=batch_size\n",
    "            )\n",
    "(x_train, y_train), (x_valid, y_valid)=datasetOBJ.load_dataset()\n",
    "num_of_classes=x_train.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98424268-ec78-4d67-aaeb-da102c5ddc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_dict=datasetOBJ.as_dictionary(x_train)\n",
    "validset_dict=datasetOBJ.as_dictionary(x_valid)\n",
    "#show_chart(chart_Type=\"barh\", data_Dictionary=trainset_dict,chart_title='Training Set',label_x='classes',label_y=\"samples\") \n",
    "#show_chart(chart_Type=\"barh\", data_Dictionary=trainset_dict,chart_title='Training Set',label_x='classes',label_y=\"samples\") \n",
    "dataset_distribution=os.path.join(prefix,'image','dataset_distribution.png')\n",
    "show_dataset_chart(trainset_dict,validset_dict,save_filename=dataset_distribution,show_legend=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbff9ac8-fb00-412b-ba43-2c40bac4b3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "_data={\n",
    "    \"dataset\":{\n",
    "        \"trainset\":{\"num_classes\":x_train.num_classes,\n",
    "                    \"num_samples\":x_train.samples,\n",
    "                    \"class_wise_count\":datasetOBJ.trainSet_as_dictionary()\n",
    "                  },\n",
    "        \"validateset\":{\"num_classes\":x_valid.num_classes,\n",
    "                    \"num_samples\":x_valid.samples,\n",
    "                    \"class_wise_count\":datasetOBJ.validateSet_as_dictionary()\n",
    "                  }        \n",
    "    }\n",
    "}\n",
    "    \n",
    "_d1=f'\\tTrainSet: Found {_data[\"dataset\"][\"trainset\"][\"num_samples\"]} images belonging to {_data[\"dataset\"][\"trainset\"][\"num_classes\"]} classes.'\n",
    "_d2=f'\\tValidateset: Found {_data[\"dataset\"][\"validateset\"][\"num_samples\"]} images belonging to {_data[\"dataset\"][\"validateset\"][\"num_classes\"]} classes.'\n",
    "\n",
    "       # images belonging to {_data[\"dataset\"][\"validateset\"][\"num_classes\"]} classes.')\n",
    "\n",
    "log_txt.print_log(_d1,print_on_screen=False)\n",
    "log_txt.print_log(_d2,print_on_screen=False)\n",
    "log_json.print_jsonlog(_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2f8bce-1ddc-4e64-a612-b129051c40a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4610f6e3-7de5-41e5-a588-0f24d172b560",
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7036e406-d3a3-41cc-9060-72ea3f21ce0f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Modeling\n",
    "- get base model\n",
    "- create your model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c32834c-d04e-4f73-881a-e16871611d3d",
   "metadata": {},
   "source": [
    "#### get base-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffba21a-d7da-43c5-ab79-4f1213221789",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_app=keras_applications()\n",
    "base_model=keras_app.get_base_model(base_architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c721ecb-a7ad-4348-aa36-0ae8c6ba8e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_txt.print_log(\"Modeling...\")\n",
    "log_txt.print_log(\"\\tbase_model: \"+ base_architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce7e410-777e-4d3b-bf28-e1c75c2357a7",
   "metadata": {},
   "source": [
    "#### create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896b0ec9-32d6-43b0-a62e-669751e2727e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "def create_model(base,num_of_classes,model_name):\n",
    "    #biasInitializer = tf.keras.initializers.HeNormal(seed=101)\n",
    "    inputShape=base_model.input_shape\n",
    "    inputShape=(inputShape[1],inputShape[2],inputShape[3])\n",
    "    inputs = keras.Input(shape=inputShape)\n",
    "    x = base(inputs, training=True)## todo\n",
    "    #x = keras.layers.Conv2D(1280*2, (3, 3), strides= (1, 1), activation=\"relu\", name=\"for_CAM\")(x)\n",
    "    x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = keras.layers.Dense(num_of_classes)(x)\n",
    "    outputs = keras.layers.Softmax()(x)\n",
    "    model=keras.Model(inputs, outputs,name=model_name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c28ace1-0a8d-4c25-80d8-1071b2103112",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=create_model(base=base_model,\n",
    "                   num_of_classes=num_of_classes,\n",
    "                   model_name=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17113352-5791-44c1-8f22-c56bc6e43be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary(x):\n",
    "    log_txt.print_log(\"\\t\"+x,print_on_screen=True)\n",
    "    #print(\"\\t\"+x)\n",
    "    x=str(x)\n",
    "    if x.startswith(\"Total params:\") or x.startswith(\"Trainable params:\") or x.startswith(\"Non-trainable params:\"):\n",
    "        token=x.split(':')\n",
    "        k=token[0]\n",
    "        v=token[1].strip()\n",
    "        _data[k]=v\n",
    "\n",
    "_data={}\n",
    "model.summary(print_fn=print_summary)\n",
    "_data={\"model\":{\"base_model\":base_architecture,\n",
    "                \"summary\":_data\n",
    "               }\n",
    "}\n",
    "log_json.print_jsonlog(_data)\n",
    "\n",
    "model_snap=os.path.join(prefix,'image','model_snap.png')\n",
    "keras.utils.plot_model(model,\n",
    "                       to_file=model_snap,\n",
    "                       show_shapes=True,\n",
    "                       show_dtype=False,\n",
    "                       show_layer_names=True,\n",
    "                       rankdir='TB',\n",
    "                       expand_nested=False,\n",
    "                       dpi=96)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7649ce0-ce46-4e66-a0a4-88f044dabe27",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e605442-3743-449c-9a5d-9bbcdcf4a4ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae07d190-b08b-4775-9910-fd34a4488d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://blog.paperspace.com/tensorflow-callbacks/\n",
    "path_csvname = os.path.join (prefix,\"CSVLogger.csv\")\n",
    "path_checkpoint = os.path.join(prefix,'ckpt','model_epoch{epoch:02d}_vLoss{val_loss:.2f}.hdf5')\n",
    "path_custom_file1 = os.path.join (prefix,\"log_batchwise.txt\")\n",
    "\n",
    "path_tensorboardLog = os.path.join (prefix,\"tensorboard_logs\")\n",
    "#path_tensorboardLog = os.path.join (prefix,'tensorboard_logs',\"scalars\" , datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "\n",
    "CSVLogger_cb = tf.keras.callbacks.CSVLogger(path_csvname, \n",
    "                             separator=',', \n",
    "                             append=True)\n",
    "\n",
    "\n",
    "TensorBoard_cb = tf.keras.callbacks.TensorBoard(log_dir=path_tensorboardLog,\n",
    "                                          histogram_freq=1,\n",
    "                                          write_graph=True,\n",
    "                                          write_images=True,\n",
    "                                          update_freq=batch_size,\n",
    "                                          #write_steps_per_second=False,\n",
    "                                          profile_batch=2,\n",
    "                                          embeddings_metadata=None)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ModelCheckpoint_cb = tf.keras.callbacks.ModelCheckpoint(filepath=path_checkpoint,\n",
    "                                                save_best_only=True, ###to save space\n",
    "                                                save_weights_only=False,\n",
    "                                                monitor='val_accuracy',\n",
    "                                                mode='max')\n",
    "\n",
    "\n",
    "\n",
    "EarlyStopping_cb = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                                   min_delta=0.001,\n",
    "                                                   patience=4,\n",
    "                                                   verbose=0, \n",
    "                                                   mode='auto',\n",
    "                                                   baseline=None,\n",
    "                                                   restore_best_weights=False)\n",
    "\n",
    "TerminateOnNaN_cb = tf.keras.callbacks.TerminateOnNaN()\n",
    "\n",
    "ReduceLROnPlateau_cb = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
    "                                                            factor=0.01,\n",
    "                                                            patience=3,\n",
    "                                                            verbose=0,\n",
    "                                                            mode='auto',\n",
    "                                                            min_delta=0.001,\n",
    "                                                            cooldown=0,\n",
    "                                                            min_lr=0)\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 15:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.0001)\n",
    "\n",
    "LearningRateScheduler_cb = tf.keras.callbacks.LearningRateScheduler(scheduler, \n",
    "                                                                    verbose=0)\n",
    "\n",
    "\n",
    "\n",
    "RemoteMonitor_cb = tf.keras.callbacks.RemoteMonitor(root='http://localhost:9000',\n",
    "                                                    path='/publish/epoch/end/',\n",
    "                                                    field='data',\n",
    "                                                    headers=None,\n",
    "                                                    send_as_json=False)\n",
    "\n",
    "\n",
    "callback_dictionary={\"CSVLogger\":{\"callback\":CSVLogger_cb},\n",
    "                     \"TensorBoard\":{\"callback\":TensorBoard_cb},\n",
    "                     \"ModelCheckpoint\":{\"callback\":ModelCheckpoint_cb},\n",
    "                     \"EarlyStopping\":{\"callback\":EarlyStopping_cb},\n",
    "                     \"TerminateOnNaN\":{\"callback\":TerminateOnNaN_cb},\n",
    "                     \"ReduceLROnPlateau\":{\"callback\":ReduceLROnPlateau_cb},\n",
    "                     \"LearningRateScheduler\":{\"callback\":LearningRateScheduler_cb},\n",
    "                     \"RemoteMonitor\":{\"callback\":RemoteMonitor_cb}}\n",
    "callbacksList=[]\n",
    "for k in callbacks_enabled:\n",
    "    if callbacks_enabled[k]:\n",
    "        callbacksList.append(callback_dictionary[k][\"callback\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaa5502-c4b7-40b8-aaa8-9d6dac20852c",
   "metadata": {},
   "source": [
    "#### monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85f2afa-7991-48b7-9961-f34359688de2",
   "metadata": {},
   "source": [
    " http://deeplearning.local:6006  \n",
    " \n",
    " - to open 6006 port: **sudu ufw allow 6006:6009**   http://deeplearning.local:6006  \n",
    " \n",
    " - to open 6006 port: **sudu ufw allow 6006:6009**  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "cd434066-acfa-44aa-812e-065fa0be2188",
   "metadata": {},
   "source": [
    "!kill $(ps aux | grep 'tensorboard_logs' | awk '{print $2}')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "08608440-5371-4325-9cc7-399c69118b61",
   "metadata": {},
   "source": [
    "log_txt.print_log('\\nTensorboard setup...')\n",
    "log_txt.print_log('\\tCopy and execute:')\n",
    "log_txt.print_log(f'\\t  tensorboard --logdir={path_tensorboardLog} --host 0.0.0.0 --port 6006')\n",
    "log_txt.print_log('\\tthen open this in browser:')\n",
    "log_txt.print_log('\\t  http://deeplearning.local:6006')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "732dd5a3-f527-4cbc-855a-1147948e135f",
   "metadata": {},
   "source": [
    "# Clear any logs from previous runs\n",
    "!rm -rf $path_tensorboardLog\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=$path_tensorboardLog --host 0.0.0.0 --port 6006"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d82c447-2429-4b8e-a018-d5daf367a92d",
   "metadata": {},
   "source": [
    "#### training in progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea9b309-6ee3-4796-aa47-2ed0f041dc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "historyIndex=0\n",
    "index_finish=0\n",
    "\n",
    "steps=epochs\n",
    "_metrics=['MeanSquaredError','AUC','Precision','Recall','accuracy']\n",
    "histories={}\n",
    "_data={\"training\":{}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8643d83-8c1c-420e-9a20-28b6a551209e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=_metrics,\n",
    "    loss_weights=None,\n",
    "    weighted_metrics=None,\n",
    "    run_eagerly=None,\n",
    "    steps_per_execution=None\n",
    ")\n",
    "class_weight_dict=None\n",
    "\n",
    "_data_compile={\"optimizer\":\"adam\",\n",
    "               \"loss\":\"CategoricalCrossentropy\",\n",
    "               \"metrics\":[\"accuracy\"],\n",
    "               \"loss_weights\":\"None\",\n",
    "               \"weighted_metrics\":\"None\"\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b116857-062a-423e-8c35-d717c43dc4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_txt.print_log(\"Training...\")\n",
    "log_txt.print_log(\"\\tcompile:\")\n",
    "log_txt.print_log(\"\\t\\toptimizer: \"+_data_compile[\"optimizer\"])\n",
    "log_txt.print_log(\"\\t\\tloss: \"+_data_compile[\"loss\"])\n",
    "log_txt.print_log(\"\\t\\tloss_weights: \"+_data_compile[\"loss_weights\"])   \n",
    "log_txt.print_log(\"\\t\\tweighted_metrics: \"+_data_compile[\"weighted_metrics\"]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e69c3e-2ed2-4488-aa47-b13c699bddcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_start=index_finish\n",
    "index_finish+=steps\n",
    "historyIndex+=1\n",
    "_data={\"training\":{}}\n",
    "_data_epoch={\"index\":historyIndex,\n",
    "             \"epoch_from\":index_start,\n",
    "             \"epoch_to\":index_finish\n",
    "            }\n",
    "_data[\"training\"][\"session\"]=_data_epoch\n",
    "_data[\"training\"][\"compiler\"]=_data_compile\n",
    "\n",
    "log_txt.print_log(\"\\tsession:\")\n",
    "log_txt.print_log(f\"\\t\\tindex:{historyIndex}, epoch from {index_start} to {index_finish}\")\n",
    "log_json.print_jsonlog(_data)\n",
    "\n",
    "try:\n",
    "    train_session = model.fit(\n",
    "        x=x_train,\n",
    "        #y=y_train,\n",
    "        #validation_split=0.0,\n",
    "        validation_data=(x_valid),\n",
    "        batch_size=batch_size,\n",
    "        epochs=index_finish,\n",
    "        callbacks=callbacksList,\n",
    "        verbose=1,\n",
    "        shuffle=True,\n",
    "        class_weight=class_weight_dict,\n",
    "        sample_weight=None,\n",
    "        initial_epoch=index_start,\n",
    "        steps_per_epoch=None,\n",
    "        validation_steps=None,\n",
    "        validation_batch_size=None,\n",
    "        validation_freq=1,\n",
    "        max_queue_size=10,\n",
    "        workers=1,\n",
    "        use_multiprocessing=False)\n",
    "    \n",
    "    #histories[historyIndex]=train_session.history\n",
    "    log_txt.print_log(\"\\tresults:\",print_on_screen=False)\n",
    "    log_txt.print_log(str(train_session.history),print_on_screen=False)\n",
    "\n",
    "    if len(train_session.epoch)<index_finish-index_start:\n",
    "        log_txt.print_log('\\tTraining session completed at epoch '+str(len(train_session.epoch)))\n",
    "        _data={\"stopped_at_epoch\":len(train_session.epoch)}\n",
    "        log_json.print_jsonlog(_data)\n",
    "        \n",
    "    training_performance = os.path.join (path_to_image,\"training_performance.png\")\n",
    "    show_training_performance(path_csvname=path_csvname,save_filename=training_performance)\n",
    "\n",
    "except Exception as e:\n",
    "    if e.error_code==8:\n",
    "        _exception={\"error\":\n",
    "                    {\"code\":e.error_code,\n",
    "                     \"desc\":\"Resource exhausted\",\n",
    "                     \"fix\":[\"reduce batch size\"]\n",
    "                    }\n",
    "                   }\n",
    "    else:\n",
    "        _exception={\"error\":\n",
    "                    {\"code\":e.error_code,\n",
    "                     \"desc\":str(e),\n",
    "                    }\n",
    "                   }\n",
    "\n",
    "    log_txt.print_log(str(_exception))\n",
    "    log_json.print_jsonlog(_exception)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8374917-736f-4331-ac3d-2a79aa1190f8",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094fcf69-89ad-468d-9af9-f8a57ca3df00",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = os.path.join (prefix,\"saved_model\")\n",
    "savedModel = os.path.join(prefix, \"saved_model\")\n",
    "savedWeights = os.path.join(prefix,\"saved_weights\",\"weights\")\n",
    "\n",
    "\n",
    "log_txt.print_log(\"Saving...\")\n",
    "model.save(savedModel)\n",
    "log_txt.print_log(\"\\tComplete model saved:\"+ str(savedModel))\n",
    "model.save_weights(savedWeights)\n",
    "log_txt.print_log(\"\\tWeights saved:\"+ str(savedWeights))\n",
    "\n",
    "_data={\"saved\":{\"full\":str(savedModel),\n",
    "              \"weights\":str(savedWeights)}}\n",
    "log_json.print_jsonlog(_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f608d5c4-f553-4353-b631-c6072d4362d0",
   "metadata": {},
   "source": [
    "## confusion metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31208a7f-ed7f-4549-9791-76285ad70a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "fname = os.path.join (prefix,\"saved_model\")\n",
    "savedModel = os.path.join(prefix, \"saved_model\")\n",
    "savedWeights = os.path.join(prefix,\"saved_weights\",\"weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9518432-d816-466a-bb70-54c497351023",
   "metadata": {},
   "outputs": [],
   "source": [
    "model= keras.models.load_model(savedModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020b8609-c63b-4ae6-b47b-702586b465bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions =  model.predict(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9b1485-67b5-4852-9c34-eaef6c1141ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "y_pred = pred=np.argmax(predictions,axis=1)\n",
    "y_test = y_valid \n",
    "labels =list(trainset_dict.keys())\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0840e4d2-9a00-4194-95d7-b920513dde65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
