{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "661e7371-4255-4a8f-a377-2ec87c35d6f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Trainer v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1abc7af-0ef6-4d26-be03-1d4ccdc8fadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "import sys, getopt, os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\" # or any {'0', '1', '2','3'}\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "from lib.lib_utils import utilities\n",
    "from lib.lib_logger import logger\n",
    "from lib.lib_keras import keras_applications\n",
    "from lib.lib_dataset import Dataset\n",
    "from lib.lib_plot import show_training_performance,show_dataset_chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9200413-87be-4ac0-b66b-82d71d1a7295",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Definitions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca235d0d-e83c-4766-8d3b-1852a0f0b85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(base,num_of_classes,model_name):\n",
    "    #biasInitializer = tf.keras.initializers.HeNormal(seed=101)\n",
    "    inputShape=base_model.input_shape\n",
    "    inputShape=(inputShape[1],inputShape[2],inputShape[3])\n",
    "    inputs = keras.Input(shape=inputShape)\n",
    "    x = base(inputs, training=True)## todo\n",
    "    #x = keras.layers.Conv2D(1280*2, (3, 3), strides= (1, 1), activation=\"relu\", name=\"for_CAM\")(x)\n",
    "    x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = keras.layers.Dense(num_of_classes)(x)\n",
    "    outputs = keras.layers.Softmax()(x)\n",
    "    model=keras.Model(inputs, outputs,name=model_name)\n",
    "    return model\n",
    "\n",
    "def print_summary(x):\n",
    "    log_txt.print_log(\"\\t\"+x,print_on_screen=True)\n",
    "    #print(\"\\t\"+x)\n",
    "    x=str(x)\n",
    "    if x.startswith(\"Total params:\") or x.startswith(\"Trainable params:\") or x.startswith(\"Non-trainable params:\"):\n",
    "        token=x.split(':')\n",
    "        k=token[0]\n",
    "        v=token[1].strip()\n",
    "        _data[k]=v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced65b35-390b-4775-b29f-4b60d1db9036",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f452101d-9656-4117-9d2b-d7f7dd222e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(argv):\n",
    "    util=utilities()\n",
    "  \n",
    "    try:\n",
    "        opts, args = getopt.getopt(argv,\"hi:\",[\"ifile=\",\"ofile=\"])\n",
    "        \n",
    "    except getopt.GetoptError:\n",
    "        print ('train.py -i <inputfile>')\n",
    "        sys.exit(2)\n",
    "        \n",
    "    for opt, arg in opts:\n",
    "        if opt == '-h':\n",
    "            print ('train.py -i <conf_yourFilename.json>')\n",
    "            sys.exit()\n",
    "        elif opt in (\"-i\", \"--ifile\"):\n",
    "            inputfile = arg\n",
    "\n",
    "    print ('Input file is \"', inputfile)\n",
    "    USER_PARA=util.load_JSON_file('my_workspace/exp00_montgomeryset.json')\n",
    "    #USER_PARA=util.load_JSON_file('my_workspace/exp01_covid19_mobilenetv2.json')\n",
    "    #USER_PARA=util.load_JSON_file(inputfile)\n",
    "    \n",
    "    # step: Definitions\n",
    "    prefix=USER_PARA['prefix']\n",
    "    project_workspace=os.path.join('my_workspace',prefix)\n",
    "\n",
    "    color_mode=USER_PARA[\"colorMode\"]\n",
    "    imgChannel= 1 if color_mode=='grayscale' else 3\n",
    "    image_size=(USER_PARA['imgHeight'],USER_PARA['imgWidth'])\n",
    "\n",
    "    image_shape=(USER_PARA['imgHeight'],USER_PARA['imgWidth'],imgChannel)\n",
    "\n",
    "    logFilenameJSON=os.path.join(project_workspace,\"log.json\")\n",
    "    logFilenameTXT=os.path.join(project_workspace,\"log.txt\")\n",
    "\n",
    "    base_architecture=USER_PARA['architecture']\n",
    "\n",
    "    root_dataset=USER_PARA[\"root_dataset\"]\n",
    "\n",
    "    batch_size=USER_PARA[\"batch_size\"]\n",
    "    dataset_tag=USER_PARA['dataset_tag']\n",
    "    epochs=USER_PARA['epochs']\n",
    "\n",
    "    callbacks_enabled={\"CSVLogger\":True,\n",
    "                         \"TensorBoard\":True,\n",
    "                         \"ModelCheckpoint\":True,\n",
    "                         \"EarlyStopping\":True,\n",
    "                         \"TerminateOnNaN\":True,\n",
    "                         \"ReduceLROnPlateau\":True,\n",
    "                         \"LearningRateScheduler\":False,\n",
    "                         \"RemoteMonitor\":False}\n",
    "    _exception={}            \n",
    "    \n",
    "    \n",
    "    # step: directory structure and filenames\n",
    "    if not os.path.isdir(project_workspace):\n",
    "        os.mkdir(project_workspace)\n",
    "\n",
    "    path_to_image=os.path.join(project_workspace,'image')\n",
    "    if not os.path.isdir(path_to_image):\n",
    "        os.mkdir(path_to_image)\n",
    "\n",
    "    log_json=logger(logFilenameJSON)\n",
    "    log_txt=logger(logFilenameTXT)\n",
    "\n",
    "    log_txt.print_log(\"Getting Starting...\",overwrite=True)\n",
    "    log_txt.print_log('\\tprefix:'+prefix)\n",
    "\n",
    "    log_json.print_jsonlog({'prefix':prefix},overwrite=True)\n",
    "    \n",
    "    # step: Dataset\n",
    "    log_txt.print_log('Loading Dataset...')\n",
    "    datasetOBJ=Dataset(dataset_tag=dataset_tag,\n",
    "                 path_to_dataset=root_dataset,\n",
    "                 image_size=image_size, \n",
    "                 color_mode=color_mode, \n",
    "                 batch_size=batch_size\n",
    "                )\n",
    "    (x_train, y_train), (x_valid, y_valid)=datasetOBJ.load_dataset()\n",
    "    num_of_classes=x_train.num_classes\n",
    "    \n",
    "    trainset_dict=datasetOBJ.as_dictionary(x_train)\n",
    "    validset_dict=datasetOBJ.as_dictionary(x_valid)\n",
    "    #show_chart(chart_Type=\"barh\", data_Dictionary=trainset_dict,chart_title='Training Set',label_x='classes',label_y=\"samples\") \n",
    "    #show_chart(chart_Type=\"barh\", data_Dictionary=trainset_dict,chart_title='Training Set',label_x='classes',label_y=\"samples\") \n",
    "    dataset_distribution=os.path.join(project_workspace,'image','dataset_distribution.png')\n",
    "    show_dataset_chart(trainset_dict,validset_dict,save_filename=dataset_distribution,show_legend=True )\n",
    "    \n",
    "    _data={        \n",
    "        \"dataset\":{\n",
    "            \"trainset\":{\"num_classes\":x_train.num_classes,\n",
    "                        \"num_samples\":x_train.samples,\n",
    "                        \"class_wise_count\":datasetOBJ.trainSet_as_dictionary()\n",
    "                      },\n",
    "            \"validateset\":{\"num_classes\":x_valid.num_classes,\n",
    "                        \"num_samples\":x_valid.samples,\n",
    "                        \"class_wise_count\":datasetOBJ.validateSet_as_dictionary()\n",
    "                      }        \n",
    "            }\n",
    "        }\n",
    "\n",
    "    _d1=f'\\tTrainSet: Found {_data[\"dataset\"][\"trainset\"][\"num_samples\"]} images belonging to {_data[\"dataset\"][\"trainset\"][\"num_classes\"]} classes.'\n",
    "    _d2=f'\\tValidateset: Found {_data[\"dataset\"][\"validateset\"][\"num_samples\"]} images belonging to {_data[\"dataset\"][\"validateset\"][\"num_classes\"]} classes.'\n",
    "\n",
    "    log_txt.print_log(_d1,print_on_screen=False)\n",
    "    log_txt.print_log(_d2,print_on_screen=False)\n",
    "    log_json.print_jsonlog(_data)\n",
    "    \n",
    "\n",
    "    # step: Modeling\n",
    "    keras_app=keras_applications()\n",
    "    base_model=keras_app.get_base_model(base_architecture)\n",
    "\n",
    "    log_txt.print_log(\"Modeling...\")\n",
    "    log_txt.print_log(\"\\tbase_model: \"+ base_architecture)\n",
    "    \n",
    "    ## create model\n",
    "    model=create_model(base=base_model,\n",
    "                   num_of_classes=num_of_classes,\n",
    "                   model_name=prefix)\n",
    "    \n",
    "    _data={}\n",
    "    model.summary(print_fn=print_summary)\n",
    "    _data={\"model\":{\"base_model\":base_architecture,\n",
    "                    \"summary\":_data\n",
    "                   }\n",
    "    }\n",
    "    log_json.print_jsonlog(_data)\n",
    "\n",
    "    model_snap=os.path.join(project_workspace,'image','model_snap.png')\n",
    "    keras.utils.plot_model(model,\n",
    "                           to_file=model_snap,\n",
    "                           show_shapes=True,\n",
    "                           show_dtype=False,\n",
    "                           show_layer_names=True,\n",
    "                           rankdir='TB',\n",
    "                           expand_nested=False,\n",
    "                           dpi=96)    \n",
    "        \n",
    "    #callbacks\n",
    "    # https://blog.paperspace.com/tensorflow-callbacks/\n",
    "    path_csvname = os.path.join (project_workspace,\"CSVLogger.csv\")\n",
    "    path_checkpoint = os.path.join(project_workspace,'ckpt','model_epoch{epoch:02d}_vLoss{val_loss:.2f}.hdf5')\n",
    "    path_custom_file1 = os.path.join (project_workspace,\"log_batchwise.txt\")\n",
    "\n",
    "    path_tensorboardLog = os.path.join (project_workspace,\"tensorboard_logs\")\n",
    "    #path_tensorboardLog = os.path.join (project_workspace,'tensorboard_logs',\"scalars\" , datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "\n",
    "    CSVLogger_cb = tf.keras.callbacks.CSVLogger(path_csvname, \n",
    "                                 separator=',', \n",
    "                                 append=True)\n",
    "\n",
    "\n",
    "    TensorBoard_cb = tf.keras.callbacks.TensorBoard(log_dir=path_tensorboardLog,\n",
    "                                              histogram_freq=1,\n",
    "                                              write_graph=True,\n",
    "                                              write_images=True,\n",
    "                                              update_freq=batch_size,\n",
    "                                              #write_steps_per_second=False,\n",
    "                                              profile_batch=2,\n",
    "                                              embeddings_metadata=None)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ModelCheckpoint_cb = tf.keras.callbacks.ModelCheckpoint(filepath=path_checkpoint,\n",
    "                                                    save_best_only=True, ###to save space\n",
    "                                                    save_weights_only=False,\n",
    "                                                    monitor='val_accuracy',\n",
    "                                                    mode='max')\n",
    "\n",
    "\n",
    "\n",
    "    EarlyStopping_cb = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                                       min_delta=0.001,\n",
    "                                                       patience=4,\n",
    "                                                       verbose=0, \n",
    "                                                       mode='auto',\n",
    "                                                       baseline=None,\n",
    "                                                       restore_best_weights=False)\n",
    "\n",
    "    TerminateOnNaN_cb = tf.keras.callbacks.TerminateOnNaN()\n",
    "\n",
    "    ReduceLROnPlateau_cb = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
    "                                                                factor=0.01,\n",
    "                                                                patience=3,\n",
    "                                                                verbose=0,\n",
    "                                                                mode='auto',\n",
    "                                                                min_delta=0.001,\n",
    "                                                                cooldown=0,\n",
    "                                                                min_lr=0)\n",
    "\n",
    "    def scheduler(epoch, lr):\n",
    "        if epoch < 15:\n",
    "            return lr\n",
    "        else:\n",
    "            return lr * tf.math.exp(-0.0001)\n",
    "\n",
    "    LearningRateScheduler_cb = tf.keras.callbacks.LearningRateScheduler(scheduler, \n",
    "                                                                        verbose=0)\n",
    "\n",
    "\n",
    "\n",
    "    RemoteMonitor_cb = tf.keras.callbacks.RemoteMonitor(root='http://localhost:9000',\n",
    "                                                        path='/publish/epoch/end/',\n",
    "                                                        field='data',\n",
    "                                                        headers=None,\n",
    "                                                        send_as_json=False)\n",
    "\n",
    "\n",
    "    callback_dictionary={\"CSVLogger\":{\"callback\":CSVLogger_cb},\n",
    "                         \"TensorBoard\":{\"callback\":TensorBoard_cb},\n",
    "                         \"ModelCheckpoint\":{\"callback\":ModelCheckpoint_cb},\n",
    "                         \"EarlyStopping\":{\"callback\":EarlyStopping_cb},\n",
    "                         \"TerminateOnNaN\":{\"callback\":TerminateOnNaN_cb},\n",
    "                         \"ReduceLROnPlateau\":{\"callback\":ReduceLROnPlateau_cb},\n",
    "                         \"LearningRateScheduler\":{\"callback\":LearningRateScheduler_cb},\n",
    "                         \"RemoteMonitor\":{\"callback\":RemoteMonitor_cb}}\n",
    "    callbacksList=[]\n",
    "    for k in callbacks_enabled:\n",
    "        if callbacks_enabled[k]:\n",
    "            callbacksList.append(callback_dictionary[k][\"callback\"])\n",
    "\n",
    "    # step: training\n",
    "    historyIndex=0\n",
    "    index_finish=0\n",
    "\n",
    "    steps=epochs\n",
    "    _metrics=['MeanSquaredError','AUC','Precision','Recall','accuracy']\n",
    "    histories={}\n",
    "    _data={\"training\":{}}\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        loss=keras.losses.CategoricalCrossentropy(),\n",
    "        metrics=_metrics,\n",
    "        loss_weights=None,\n",
    "        weighted_metrics=None,\n",
    "        run_eagerly=None,\n",
    "        steps_per_execution=None\n",
    "    )\n",
    "    class_weight_dict=None\n",
    "\n",
    "    _data_compile={\"optimizer\":\"adam\",\n",
    "                   \"loss\":\"CategoricalCrossentropy\",\n",
    "                   \"metrics\":[\"accuracy\"],\n",
    "                   \"loss_weights\":\"None\",\n",
    "                   \"weighted_metrics\":\"None\"\n",
    "                  }\n",
    "\n",
    "    log_txt.print_log(\"Training...\")\n",
    "    log_txt.print_log(\"\\tcompile:\")\n",
    "    log_txt.print_log(\"\\t\\toptimizer: \"+_data_compile[\"optimizer\"])\n",
    "    log_txt.print_log(\"\\t\\tloss: \"+_data_compile[\"loss\"])\n",
    "    log_txt.print_log(\"\\t\\tloss_weights: \"+_data_compile[\"loss_weights\"])   \n",
    "    log_txt.print_log(\"\\t\\tweighted_metrics: \"+_data_compile[\"weighted_metrics\"])     \n",
    "    \n",
    "    index_start=index_finish\n",
    "    index_finish+=steps\n",
    "    historyIndex+=1\n",
    "    _data={\"training\":{}}\n",
    "    _data_epoch={\"index\":historyIndex,\n",
    "                 \"epoch_from\":index_start,\n",
    "                 \"epoch_to\":index_finish\n",
    "                }\n",
    "    _data[\"training\"][\"session\"]=_data_epoch\n",
    "    _data[\"training\"][\"compiler\"]=_data_compile\n",
    "\n",
    "    log_txt.print_log(\"\\tsession:\")\n",
    "    log_txt.print_log(f\"\\t\\tindex:{historyIndex}, epoch from {index_start} to {index_finish}\")\n",
    "    log_json.print_jsonlog(_data)\n",
    "\n",
    "    try:\n",
    "        train_session = model.fit(\n",
    "            x=x_train,\n",
    "            #y=y_train,\n",
    "            #validation_split=0.0,\n",
    "            validation_data=(x_valid),\n",
    "            batch_size=batch_size,\n",
    "            epochs=index_finish,\n",
    "            callbacks=callbacksList,\n",
    "            verbose=1,\n",
    "            shuffle=True,\n",
    "            class_weight=class_weight_dict,\n",
    "            sample_weight=None,\n",
    "            initial_epoch=index_start,\n",
    "            steps_per_epoch=None,\n",
    "            validation_steps=None,\n",
    "            validation_batch_size=None,\n",
    "            validation_freq=1,\n",
    "            max_queue_size=10,\n",
    "            workers=1,\n",
    "            use_multiprocessing=False)\n",
    "\n",
    "        #histories[historyIndex]=train_session.history\n",
    "        log_txt.print_log(\"\\tresults:\",print_on_screen=False)\n",
    "        log_txt.print_log(str(train_session.history),print_on_screen=False)\n",
    "\n",
    "        if len(train_session.epoch)<index_finish-index_start:\n",
    "            log_txt.print_log('\\tTraining session completed at epoch '+str(len(train_session.epoch)))\n",
    "            _data={\"stopped_at_epoch\":len(train_session.epoch)}\n",
    "            log_json.print_jsonlog(_data)\n",
    "\n",
    "        training_performance = os.path.join (path_to_image,\"training_performance.png\")\n",
    "        show_training_performance(path_csvname=path_csvname,save_filename=training_performance)\n",
    "\n",
    "    except Exception as e:\n",
    "        if e.error_code==8:\n",
    "            _exception={\"error\":\n",
    "                        {\"code\":e.error_code,\n",
    "                         \"desc\":\"Resource exhausted\",\n",
    "                         \"fix\":[\"reduce batch size\"]\n",
    "                        }\n",
    "                       }\n",
    "        else:\n",
    "            _exception={\"error\":\n",
    "                        {\"code\":e.error_code,\n",
    "                         \"desc\":str(e),\n",
    "                        }\n",
    "                       }\n",
    "\n",
    "        log_txt.print_log(str(_exception))\n",
    "        log_json.print_jsonlog(_exception)    \n",
    "        \n",
    "    # step: Save the model\n",
    "    fname = os.path.join (project_workspace,\"saved_model\")\n",
    "    savedModel = os.path.join(project_workspace, \"saved_model\")\n",
    "    savedWeights = os.path.join(project_workspace,\"saved_weights\",\"weights\")\n",
    "\n",
    "\n",
    "    log_txt.print_log(\"Saving...\")\n",
    "    model.save(savedModel)\n",
    "    log_txt.print_log(\"\\tComplete model saved:\"+ str(savedModel))\n",
    "    model.save_weights(savedWeights)\n",
    "    log_txt.print_log(\"\\tWeights saved:\"+ str(savedWeights))\n",
    "\n",
    "    _data={\"saved\":{\"full\":str(savedModel),\n",
    "                  \"weights\":str(savedWeights)}}\n",
    "    log_json.print_jsonlog(_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3177bbe-826d-4f2a-94ea-e15bd8130e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(sys.argv[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754341f7-3313-474d-84c2-fac2cba10d14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
